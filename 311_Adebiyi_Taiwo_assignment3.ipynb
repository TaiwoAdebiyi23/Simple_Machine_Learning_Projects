{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eee219a-4757-47e0-95f7-fb9a9b982917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a20b3e4-bac4-4882-a1db-fa4b5681b915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file ...\n",
      "done \n",
      "\n",
      "Removing rows with missing data ...\n",
      "done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preliminaries\n",
    "# ---------------------------------------------\n",
    "print('Loading data from file ...')  \n",
    "dataset = pd.read_csv('winequality-white.csv')\n",
    "print('done \\n')\n",
    "\n",
    "print('Removing rows with missing data ...')\n",
    "dataset = dataset.dropna() \n",
    "print('done \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed631330-c21a-4389-95bc-a2a74e613fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rows from the dataset (top and bottom five):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "4893            6.2              0.21         0.29             1.6      0.039   \n",
       "4894            6.6              0.32         0.36             8.0      0.047   \n",
       "4895            6.5              0.24         0.19             1.2      0.041   \n",
       "4896            5.5              0.29         0.30             1.1      0.022   \n",
       "4897            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "4893                 24.0                  92.0  0.99114  3.27       0.50   \n",
       "4894                 57.0                 168.0  0.99490  3.15       0.46   \n",
       "4895                 30.0                 111.0  0.99254  2.99       0.46   \n",
       "4896                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "4897                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "      alcohol  quality  \n",
       "4893     11.2        6  \n",
       "4894      9.6        5  \n",
       "4895      9.4        6  \n",
       "4896     12.8        7  \n",
       "4897     11.8        6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Previewing Data\n",
    "print('Sample rows from the dataset (top and bottom five):')  # Spot checks\n",
    "display(dataset.head(5))\n",
    "display(dataset.tail(5))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5155d27-ae3f-4a1b-ba8d-ba50f8d6e75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading list of problem variables X and Y...\n",
      "done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting the X(features) and y(target) \n",
    "#In this case, the features: \n",
    "#(fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, density, pH, sulphates,and alcohol) \n",
    "#will be used to predict FlowPattern\n",
    "print('Reading list of problem variables X and Y...')\n",
    "X_name = [ 'fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', \n",
    "          'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol'] # columns to focus on as predictors\n",
    "X = dataset[X_name]   # only keep these columns as features\n",
    "y_name = 'quality'     # column to focus on as target\n",
    "y = dataset[y_name]   # only keep this column as label \n",
    "print('done \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bed190e-e8e3-4183-b6c5-4acd253058e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioning data into parts: formative (for development) and summative (for testing) ...\n",
      "done with setting aside data for testing\n"
     ]
    }
   ],
   "source": [
    "# Setting up a classification problem\n",
    "\n",
    "# Split data into training and testing datasets\n",
    "\n",
    "print('Partitioning data into parts: formative (for development) and summative (for testing) ...')\n",
    "test_pct = 0.20   # reserve 20% of the data points for testing performance\n",
    "seed = 42          # specifying the seed allows for repeatability\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_pct, random_state=seed)\n",
    "print('done with setting aside data for testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6959a51-5ebd-4411-a8fe-e64f8fc8674b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading list of scoring methods to use during model development ...\n",
      "done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "\n",
    "# Chose the formative scoring method\n",
    "\n",
    "print('Reading list of scoring methods to use during model development ...')\n",
    "scoring = 'accuracy'\n",
    "print('done \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4978186-a37a-4d24-be34-a05071ac0650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading list of algorithms to train ...\n",
      "[('MLP_1', MLPClassifier(hidden_layer_sizes=(70, 70), learning_rate_init=0.1, max_iter=20,\n",
      "              random_state=42, verbose=10)), ('MLP_2', MLPClassifier(alpha=0.002, hidden_layer_sizes=(50, 50, 50),\n",
      "              learning_rate_init=0.3, max_iter=30, random_state=42, verbose=10)), ('MLP_3', MLPClassifier(alpha=0.05, hidden_layer_sizes=(60, 60, 60),\n",
      "              learning_rate_init=0.5, max_iter=50, random_state=42, verbose=10))]\n",
      "done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 1\n",
    "# Design the classifier neural network\n",
    "# Chose the Algorithms\n",
    "\n",
    "seed = 42 # setting the seed allows for repeatability\n",
    "\n",
    "print('Reading list of algorithms to train ...')\n",
    "models = []\n",
    "#mlp = MLPClassifier(hidden_layer_sizes=(30,30), activation = 'relu', alpha=0.005, learning_rate_init=0.1 , max_iter=20, random_state=42, verbose=10,)\n",
    "models.append(( 'MLP_1', MLPClassifier(hidden_layer_sizes=(70,70), activation = 'relu', alpha=0.0001, learning_rate_init=0.1 , \n",
    "                                       max_iter=20, random_state=42, verbose=10,)) )\n",
    "models.append(( 'MLP_2', MLPClassifier(hidden_layer_sizes=(50,50,50), activation = 'relu', alpha=0.002, learning_rate_init=0.3, \n",
    "                                       max_iter=30, random_state=42, verbose=10,)))\n",
    "models.append(( 'MLP_3', MLPClassifier(hidden_layer_sizes=(60,60,60), activation = 'relu', alpha=0.05, learning_rate_init=0.5, \n",
    "                                       max_iter=50, random_state=42, verbose=10,)))\n",
    "print(models)\n",
    "print('done \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc6c8789-d46f-4870-9500-79dbe722b137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ++ NOW WORKING ON ALGORITHM MLP_1 ++\n",
      "Splitting data into 5 folds\n",
      "Training model on each split ...\n",
      "Iteration 1, loss = 8.27411351\n",
      "Iteration 2, loss = 1.37084187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 1.29283614\n",
      "Iteration 4, loss = 1.28403020\n",
      "Iteration 5, loss = 1.27211429\n",
      "Iteration 6, loss = 1.25865449\n",
      "Iteration 7, loss = 1.25534199\n",
      "Iteration 8, loss = 1.25251031\n",
      "Iteration 9, loss = 1.24689365\n",
      "Iteration 10, loss = 1.24873412\n",
      "Iteration 11, loss = 1.24520102\n",
      "Iteration 12, loss = 1.25624579\n",
      "Iteration 13, loss = 1.25276250\n",
      "Iteration 14, loss = 1.25124502\n",
      "Iteration 15, loss = 1.26922609\n",
      "Iteration 16, loss = 1.24724925\n",
      "Iteration 17, loss = 1.23596862\n",
      "Iteration 18, loss = 1.23909679\n",
      "Iteration 19, loss = 1.24166524\n",
      "Iteration 20, loss = 1.23388548\n",
      "[CV] END ................................ score: (test=0.444) total time=   0.9s\n",
      "Iteration 1, loss = 6.66073191\n",
      "Iteration 2, loss = 1.32394567\n",
      "Iteration 3, loss = 1.31065670\n",
      "Iteration 4, loss = 1.30915616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 1.30356741\n",
      "Iteration 6, loss = 1.29909064\n",
      "Iteration 7, loss = 1.30201436\n",
      "Iteration 8, loss = 1.30041553\n",
      "Iteration 9, loss = 1.29991461\n",
      "Iteration 10, loss = 1.29938755\n",
      "Iteration 11, loss = 1.30491803\n",
      "Iteration 12, loss = 1.29781664\n",
      "Iteration 13, loss = 1.30074827\n",
      "Iteration 14, loss = 1.30222259\n",
      "Iteration 15, loss = 1.29966010\n",
      "Iteration 16, loss = 1.30126271\n",
      "Iteration 17, loss = 1.30083111\n",
      "Iteration 18, loss = 1.30391769\n",
      "Iteration 19, loss = 1.30042454\n",
      "Iteration 20, loss = 1.30158801\n",
      "[CV] END ................................ score: (test=0.420) total time=   0.5s\n",
      "Iteration 1, loss = 8.11985935\n",
      "Iteration 2, loss = 1.34168901\n",
      "Iteration 3, loss = 1.31827942\n",
      "Iteration 4, loss = 1.28625502\n",
      "Iteration 5, loss = 1.27861969\n",
      "Iteration 6, loss = 1.28641936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    1.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 1.26830981\n",
      "Iteration 8, loss = 1.25605636\n",
      "Iteration 9, loss = 1.25320065\n",
      "Iteration 10, loss = 1.25255955\n",
      "Iteration 11, loss = 1.25239344\n",
      "Iteration 12, loss = 1.25591477\n",
      "Iteration 13, loss = 1.26084777\n",
      "Iteration 14, loss = 1.25003979\n",
      "Iteration 15, loss = 1.24859248\n",
      "Iteration 16, loss = 1.24736876\n",
      "Iteration 17, loss = 1.24519164\n",
      "Iteration 18, loss = 1.25035963\n",
      "Iteration 19, loss = 1.24572962\n",
      "Iteration 20, loss = 1.24553440\n",
      "[CV] END ................................ score: (test=0.474) total time=   0.5s\n",
      "Iteration 1, loss = 9.07495662\n",
      "Iteration 2, loss = 1.31238087\n",
      "Iteration 3, loss = 1.29511538\n",
      "Iteration 4, loss = 1.29108314\n",
      "Iteration 5, loss = 1.29148326\n",
      "Iteration 6, loss = 1.29213315\n",
      "Iteration 7, loss = 1.28898614\n",
      "Iteration 8, loss = 1.29320991\n",
      "Iteration 9, loss = 1.28997585\n",
      "Iteration 10, loss = 1.29101923\n",
      "Iteration 11, loss = 1.28785366\n",
      "Iteration 12, loss = 1.29069230\n",
      "Iteration 13, loss = 1.29400047\n",
      "Iteration 14, loss = 1.29048021\n",
      "Iteration 15, loss = 1.29058682\n",
      "Iteration 16, loss = 1.29004018\n",
      "Iteration 17, loss = 1.29112275\n",
      "Iteration 18, loss = 1.28907650\n",
      "Iteration 19, loss = 1.28912674\n",
      "Iteration 20, loss = 1.29332901\n",
      "[CV] END ................................ score: (test=0.442) total time=   0.5s\n",
      "Iteration 1, loss = 10.44097285\n",
      "Iteration 2, loss = 1.35506342\n",
      "Iteration 3, loss = 1.32352591\n",
      "Iteration 4, loss = 1.28902668\n",
      "Iteration 5, loss = 1.28825497\n",
      "Iteration 6, loss = 1.29113751\n",
      "Iteration 7, loss = 1.28863493\n",
      "Iteration 8, loss = 1.28776439\n",
      "Iteration 9, loss = 1.28869388\n",
      "Iteration 10, loss = 1.28878806\n",
      "Iteration 11, loss = 1.28891637\n",
      "Iteration 12, loss = 1.29202124\n",
      "Iteration 13, loss = 1.28793898\n",
      "Iteration 14, loss = 1.28808849\n",
      "Iteration 15, loss = 1.28817561\n",
      "Iteration 16, loss = 1.28785765\n",
      "Iteration 17, loss = 1.29064357\n",
      "Iteration 18, loss = 1.29450179\n",
      "Iteration 19, loss = 1.29223998\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END ................................ score: (test=0.452) total time=   0.5s\n",
      "5-fold cross validation results: [0.44387755 0.41964286 0.4744898  0.44189017 0.45210728]\n",
      "\n",
      "algorithm MLP_1 accuracy results: mean = 0.446402 (std = 0.017688)\n",
      "\n",
      " ++ NOW WORKING ON ALGORITHM MLP_2 ++\n",
      "Splitting data into 5 folds\n",
      "Training model on each split ...\n",
      "Iteration 1, loss = 10.70581341\n",
      "Iteration 2, loss = 1.40946653\n",
      "Iteration 3, loss = 1.36365749\n",
      "Iteration 4, loss = 1.35428805\n",
      "Iteration 5, loss = 1.34793139\n",
      "Iteration 6, loss = 1.34777723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 1.34646344\n",
      "Iteration 8, loss = 1.34848868\n",
      "Iteration 9, loss = 1.34684224\n",
      "Iteration 10, loss = 1.34721571\n",
      "Iteration 11, loss = 1.34705469\n",
      "Iteration 12, loss = 1.34654871\n",
      "Iteration 13, loss = 1.34537715\n",
      "Iteration 14, loss = 1.34712809\n",
      "Iteration 15, loss = 1.34800935\n",
      "Iteration 16, loss = 1.34528976\n",
      "Iteration 17, loss = 1.35036549\n",
      "Iteration 18, loss = 1.34694213\n",
      "Iteration 19, loss = 1.34665759\n",
      "Iteration 20, loss = 1.34474403\n",
      "Iteration 21, loss = 1.34276915\n",
      "Iteration 22, loss = 1.34323214\n",
      "Iteration 23, loss = 1.34581123\n",
      "Iteration 24, loss = 1.34441727\n",
      "Iteration 25, loss = 1.34439538\n",
      "Iteration 26, loss = 1.34359008\n",
      "Iteration 27, loss = 1.34332339\n",
      "Iteration 28, loss = 1.34253573\n",
      "Iteration 29, loss = 1.34283632\n",
      "Iteration 30, loss = 1.34684812\n",
      "[CV] END ................................ score: (test=0.450) total time=   0.9s\n",
      "Iteration 1, loss = 12.07481848\n",
      "Iteration 2, loss = 1.38437010\n",
      "Iteration 3, loss = 1.36104676\n",
      "Iteration 4, loss = 1.34943256\n",
      "Iteration 5, loss = 1.34850725\n",
      "Iteration 6, loss = 1.34943314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 1.34821361\n",
      "Iteration 8, loss = 1.34759213\n",
      "Iteration 9, loss = 1.34580819\n",
      "Iteration 10, loss = 1.34659682\n",
      "Iteration 11, loss = 1.34796888\n",
      "Iteration 12, loss = 1.34528015\n",
      "Iteration 13, loss = 1.34584480\n",
      "Iteration 14, loss = 1.34800642\n",
      "Iteration 15, loss = 1.35004659\n",
      "Iteration 16, loss = 1.34764380\n",
      "Iteration 17, loss = 1.34753187\n",
      "Iteration 18, loss = 1.34532028\n",
      "Iteration 19, loss = 1.34557088\n",
      "Iteration 20, loss = 1.34621611\n",
      "Iteration 21, loss = 1.34523260\n",
      "Iteration 22, loss = 1.34368769\n",
      "Iteration 23, loss = 1.34497358\n",
      "Iteration 24, loss = 1.34293240\n",
      "Iteration 25, loss = 1.34452790\n",
      "Iteration 26, loss = 1.34425097\n",
      "Iteration 27, loss = 1.34368869\n",
      "Iteration 28, loss = 1.34686683\n",
      "Iteration 29, loss = 1.34997642\n",
      "Iteration 30, loss = 1.34631184\n",
      "[CV] END ................................ score: (test=0.420) total time=   0.9s\n",
      "Iteration 1, loss = 13.72857776\n",
      "Iteration 2, loss = 1.37759816\n",
      "Iteration 3, loss = 1.36461795\n",
      "Iteration 4, loss = 1.35859649\n",
      "Iteration 5, loss = 1.35160330\n",
      "Iteration 6, loss = 1.35126465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    1.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 1.35171353\n",
      "Iteration 8, loss = 1.35026728\n",
      "Iteration 9, loss = 1.35095138\n",
      "Iteration 10, loss = 1.34990090\n",
      "Iteration 11, loss = 1.35198333\n",
      "Iteration 12, loss = 1.35052659\n",
      "Iteration 13, loss = 1.35153337\n",
      "Iteration 14, loss = 1.35140124\n",
      "Iteration 15, loss = 1.35027098\n",
      "Iteration 16, loss = 1.34978011\n",
      "Iteration 17, loss = 1.35025657\n",
      "Iteration 18, loss = 1.34827326\n",
      "Iteration 19, loss = 1.34869665\n",
      "Iteration 20, loss = 1.34995614\n",
      "Iteration 21, loss = 1.34871566\n",
      "Iteration 22, loss = 1.35267575\n",
      "Iteration 23, loss = 1.35378607\n",
      "Iteration 24, loss = 1.35389428\n",
      "Iteration 25, loss = 1.35317480\n",
      "Iteration 26, loss = 1.35090034\n",
      "Iteration 27, loss = 1.35149944\n",
      "Iteration 28, loss = 1.34603104\n",
      "Iteration 29, loss = 1.34641913\n",
      "Iteration 30, loss = 1.34975185\n",
      "[CV] END ................................ score: (test=0.490) total time=   0.9s\n",
      "Iteration 1, loss = 11.17213402\n",
      "Iteration 2, loss = 1.35390916\n",
      "Iteration 3, loss = 1.34121346\n",
      "Iteration 4, loss = 1.34417612\n",
      "Iteration 5, loss = 1.34179883\n",
      "Iteration 6, loss = 1.34080756\n",
      "Iteration 7, loss = 1.34057721\n",
      "Iteration 8, loss = 1.34046095\n",
      "Iteration 9, loss = 1.34072407\n",
      "Iteration 10, loss = 1.34052218\n",
      "Iteration 11, loss = 1.34015342\n",
      "Iteration 12, loss = 1.33931245\n",
      "Iteration 13, loss = 1.34020423\n",
      "Iteration 14, loss = 1.34559088\n",
      "Iteration 15, loss = 1.33847544\n",
      "Iteration 16, loss = 1.33907384\n",
      "Iteration 17, loss = 1.34015852\n",
      "Iteration 18, loss = 1.34063044\n",
      "Iteration 19, loss = 1.33898345\n",
      "Iteration 20, loss = 1.33963497\n",
      "Iteration 21, loss = 1.34001761\n",
      "Iteration 22, loss = 1.34323595\n",
      "Iteration 23, loss = 1.34095844\n",
      "Iteration 24, loss = 1.33975372\n",
      "Iteration 25, loss = 1.33731805\n",
      "Iteration 26, loss = 1.34271573\n",
      "Iteration 27, loss = 1.33777570\n",
      "Iteration 28, loss = 1.34441417\n",
      "Iteration 29, loss = 1.33749939\n",
      "Iteration 30, loss = 1.34146378\n",
      "[CV] END ................................ score: (test=0.442) total time=   0.9s\n",
      "Iteration 1, loss = 10.02929637\n",
      "Iteration 2, loss = 1.44066750\n",
      "Iteration 3, loss = 1.35466618\n",
      "Iteration 4, loss = 1.34637540\n",
      "Iteration 5, loss = 1.34396492\n",
      "Iteration 6, loss = 1.34197344\n",
      "Iteration 7, loss = 1.34180876\n",
      "Iteration 8, loss = 1.34064469\n",
      "Iteration 9, loss = 1.34308595\n",
      "Iteration 10, loss = 1.34045108\n",
      "Iteration 11, loss = 1.34008512\n",
      "Iteration 12, loss = 1.34112304\n",
      "Iteration 13, loss = 1.34165658\n",
      "Iteration 14, loss = 1.34151155\n",
      "Iteration 15, loss = 1.34378085\n",
      "Iteration 16, loss = 1.34109886\n",
      "Iteration 17, loss = 1.33928825\n",
      "Iteration 18, loss = 1.33740718\n",
      "Iteration 19, loss = 1.33735672\n",
      "Iteration 20, loss = 1.33970272\n",
      "Iteration 21, loss = 1.33699228\n",
      "Iteration 22, loss = 1.33816310\n",
      "Iteration 23, loss = 1.33983616\n",
      "Iteration 24, loss = 1.33804134\n",
      "Iteration 25, loss = 1.33723352\n",
      "Iteration 26, loss = 1.33941447\n",
      "Iteration 27, loss = 1.33868354\n",
      "Iteration 28, loss = 1.33644588\n",
      "Iteration 29, loss = 1.33683321\n",
      "Iteration 30, loss = 1.33891304\n",
      "[CV] END ................................ score: (test=0.452) total time=   0.9s\n",
      "5-fold cross validation results: [0.4502551  0.41964286 0.48979592 0.44189017 0.45210728]\n",
      "\n",
      "algorithm MLP_2 accuracy results: mean = 0.450738 (std = 0.022684)\n",
      "\n",
      " ++ NOW WORKING ON ALGORITHM MLP_3 ++\n",
      "Splitting data into 5 folds\n",
      "Training model on each split ...\n",
      "Iteration 1, loss = 15.14384368\n",
      "Iteration 2, loss = 7.51587303\n",
      "Iteration 3, loss = 6.04723602\n",
      "Iteration 4, loss = 5.96649215\n",
      "Iteration 5, loss = 5.82378886\n",
      "Iteration 6, loss = 5.70839154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    4.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 5.60385873\n",
      "Iteration 8, loss = 5.51091578\n",
      "Iteration 9, loss = 5.42241786\n",
      "Iteration 10, loss = 5.34312568\n",
      "Iteration 11, loss = 5.26754920\n",
      "Iteration 12, loss = 5.19508642\n",
      "Iteration 13, loss = 5.13357031\n",
      "Iteration 14, loss = 5.06489202\n",
      "Iteration 15, loss = 5.00377367\n",
      "Iteration 16, loss = 4.94700607\n",
      "Iteration 17, loss = 4.89017547\n",
      "Iteration 18, loss = 4.83535333\n",
      "Iteration 19, loss = 4.78496482\n",
      "Iteration 20, loss = 4.73253518\n",
      "Iteration 21, loss = 4.68583003\n",
      "Iteration 22, loss = 4.64852908\n",
      "Iteration 23, loss = 4.60565642\n",
      "Iteration 24, loss = 4.55160390\n",
      "Iteration 25, loss = 4.51335191\n",
      "Iteration 26, loss = 4.47777917\n",
      "Iteration 27, loss = 4.43588409\n",
      "Iteration 28, loss = 4.40108970\n",
      "Iteration 29, loss = 4.36089190\n",
      "Iteration 30, loss = 4.32754512\n",
      "Iteration 31, loss = 4.29519202\n",
      "Iteration 32, loss = 4.27395606\n",
      "Iteration 33, loss = 4.24030822\n",
      "Iteration 34, loss = 4.19898098\n",
      "Iteration 35, loss = 4.17202961\n",
      "Iteration 36, loss = 4.13833150\n",
      "Iteration 37, loss = 4.11139564\n",
      "Iteration 38, loss = 4.08860428\n",
      "Iteration 39, loss = 4.06587341\n",
      "Iteration 40, loss = 4.03661959\n",
      "Iteration 41, loss = 4.00824587\n",
      "Iteration 42, loss = 3.99089835\n",
      "Iteration 43, loss = 3.96229380\n",
      "Iteration 44, loss = 3.94305049\n",
      "Iteration 45, loss = 3.91580069\n",
      "Iteration 46, loss = 3.89626340\n",
      "Iteration 47, loss = 3.87612118\n",
      "Iteration 48, loss = 3.85259650\n",
      "Iteration 49, loss = 3.83479188\n",
      "Iteration 50, loss = 3.83029726\n",
      "[CV] END ................................ score: (test=0.296) total time=   1.6s\n",
      "Iteration 1, loss = 15.62409755\n",
      "Iteration 2, loss = 7.24395977\n",
      "Iteration 3, loss = 7.52898878\n",
      "Iteration 4, loss = 6.32315473\n",
      "Iteration 5, loss = 6.20172092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 6.07769414\n",
      "Iteration 7, loss = 5.96638799\n",
      "Iteration 8, loss = 5.86599366\n",
      "Iteration 9, loss = 5.76927458\n",
      "Iteration 10, loss = 5.68387636\n",
      "Iteration 11, loss = 5.60005194\n",
      "Iteration 12, loss = 5.52231016\n",
      "Iteration 13, loss = 5.45045806\n",
      "Iteration 14, loss = 5.37741468\n",
      "Iteration 15, loss = 5.31138303\n",
      "Iteration 16, loss = 5.24665119\n",
      "Iteration 17, loss = 5.18521950\n",
      "Iteration 18, loss = 5.12716338\n",
      "Iteration 19, loss = 5.07322206\n",
      "Iteration 20, loss = 5.01996006\n",
      "Iteration 21, loss = 4.96951750\n",
      "Iteration 22, loss = 4.92654265\n",
      "Iteration 23, loss = 4.87473474\n",
      "Iteration 24, loss = 4.83027360\n",
      "Iteration 25, loss = 4.78638178\n",
      "Iteration 26, loss = 4.74227144\n",
      "Iteration 27, loss = 4.69858094\n",
      "Iteration 28, loss = 4.66121476\n",
      "Iteration 29, loss = 4.62693859\n",
      "Iteration 30, loss = 4.58982362\n",
      "Iteration 31, loss = 4.55572875\n",
      "Iteration 32, loss = 4.51936961\n",
      "Iteration 33, loss = 4.48710622\n",
      "Iteration 34, loss = 4.45529537\n",
      "Iteration 35, loss = 4.42538591\n",
      "Iteration 36, loss = 4.39820023\n",
      "Iteration 37, loss = 4.36579757\n",
      "Iteration 38, loss = 4.33555630\n",
      "Iteration 39, loss = 4.30837567\n",
      "Iteration 40, loss = 4.28090456\n",
      "Iteration 41, loss = 4.25428275\n",
      "Iteration 42, loss = 4.22881829\n",
      "Iteration 43, loss = 4.20475862\n",
      "Iteration 44, loss = 4.18679716\n",
      "Iteration 45, loss = 4.16102579\n",
      "Iteration 46, loss = 4.13451981\n",
      "Iteration 47, loss = 4.11775574\n",
      "Iteration 48, loss = 4.09448773\n",
      "Iteration 49, loss = 4.07257157\n",
      "Iteration 50, loss = 4.05639392\n",
      "[CV] END ................................ score: (test=0.420) total time=   1.6s\n",
      "Iteration 1, loss = 21.72498998\n",
      "Iteration 2, loss = 5.97699642\n",
      "Iteration 3, loss = 6.52994851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    3.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 6.34395159\n",
      "Iteration 5, loss = 6.11750972\n",
      "Iteration 6, loss = 5.99140120\n",
      "Iteration 7, loss = 5.87340959\n",
      "Iteration 8, loss = 5.77127877\n",
      "Iteration 9, loss = 5.68001269\n",
      "Iteration 10, loss = 5.59432736\n",
      "Iteration 11, loss = 5.51529331\n",
      "Iteration 12, loss = 5.44106374\n",
      "Iteration 13, loss = 5.36608228\n",
      "Iteration 14, loss = 5.29930869\n",
      "Iteration 15, loss = 5.23964696\n",
      "Iteration 16, loss = 5.17402017\n",
      "Iteration 17, loss = 5.11326584\n",
      "Iteration 18, loss = 5.05602205\n",
      "Iteration 19, loss = 5.00129657\n",
      "Iteration 20, loss = 4.94556580\n",
      "Iteration 21, loss = 4.89805832\n",
      "Iteration 22, loss = 4.84784954\n",
      "Iteration 23, loss = 4.80333042\n",
      "Iteration 24, loss = 4.75314508\n",
      "Iteration 25, loss = 4.70999677\n",
      "Iteration 26, loss = 4.66670599\n",
      "Iteration 27, loss = 4.63170176\n",
      "Iteration 28, loss = 4.59022024\n",
      "Iteration 29, loss = 4.55212544\n",
      "Iteration 30, loss = 4.51594664\n",
      "Iteration 31, loss = 4.47916811\n",
      "Iteration 32, loss = 4.44820825\n",
      "Iteration 33, loss = 4.41336072\n",
      "Iteration 34, loss = 4.38253322\n",
      "Iteration 35, loss = 4.34555453\n",
      "Iteration 36, loss = 4.31809619\n",
      "Iteration 37, loss = 4.28462927\n",
      "Iteration 38, loss = 4.25642678\n",
      "Iteration 39, loss = 4.23066879\n",
      "Iteration 40, loss = 4.20195437\n",
      "Iteration 41, loss = 4.17610637\n",
      "Iteration 42, loss = 4.14841953\n",
      "Iteration 43, loss = 4.12604094\n",
      "Iteration 44, loss = 4.10272988\n",
      "Iteration 45, loss = 4.07416952\n",
      "Iteration 46, loss = 4.05233983\n",
      "Iteration 47, loss = 4.03052067\n",
      "Iteration 48, loss = 4.01438803\n",
      "Iteration 49, loss = 3.99177180\n",
      "Iteration 50, loss = 3.97111056\n",
      "[CV] END ................................ score: (test=0.490) total time=   1.6s\n",
      "Iteration 1, loss = 15.38909671\n",
      "Iteration 2, loss = 5.68009191\n",
      "Iteration 3, loss = 6.32557675\n",
      "Iteration 4, loss = 6.32081191\n",
      "Iteration 5, loss = 6.16489742\n",
      "Iteration 6, loss = 6.02907246\n",
      "Iteration 7, loss = 5.89993111\n",
      "Iteration 8, loss = 5.79201383\n",
      "Iteration 9, loss = 5.69142475\n",
      "Iteration 10, loss = 5.60212697\n",
      "Iteration 11, loss = 5.50978057\n",
      "Iteration 12, loss = 5.43642713\n",
      "Iteration 13, loss = 5.37061247\n",
      "Iteration 14, loss = 5.29824673\n",
      "Iteration 15, loss = 5.23081957\n",
      "Iteration 16, loss = 5.14931029\n",
      "Iteration 17, loss = 5.09883687\n",
      "Iteration 18, loss = 5.04012062\n",
      "Iteration 19, loss = 4.97941843\n",
      "Iteration 20, loss = 4.92551392\n",
      "Iteration 21, loss = 4.87547567\n",
      "Iteration 22, loss = 4.82539821\n",
      "Iteration 23, loss = 4.77206946\n",
      "Iteration 24, loss = 4.73214905\n",
      "Iteration 25, loss = 4.70441667\n",
      "Iteration 26, loss = 4.65401517\n",
      "Iteration 27, loss = 4.59683202\n",
      "Iteration 28, loss = 4.55985659\n",
      "Iteration 29, loss = 4.52229113\n",
      "Iteration 30, loss = 4.48811793\n",
      "Iteration 31, loss = 4.45359881\n",
      "Iteration 32, loss = 4.41544185\n",
      "Iteration 33, loss = 4.38579009\n",
      "Iteration 34, loss = 4.35424121\n",
      "Iteration 35, loss = 4.33088745\n",
      "Iteration 36, loss = 4.31119847\n",
      "Iteration 37, loss = 4.31274740\n",
      "Iteration 38, loss = 4.27403278\n",
      "Iteration 39, loss = 4.23062757\n",
      "Iteration 40, loss = 4.23434280\n",
      "Iteration 41, loss = 4.19473636\n",
      "Iteration 42, loss = 4.14107248\n",
      "Iteration 43, loss = 4.10752480\n",
      "Iteration 44, loss = 4.07977105\n",
      "Iteration 45, loss = 4.07021851\n",
      "Iteration 46, loss = 4.03941521\n",
      "Iteration 47, loss = 4.01832275\n",
      "Iteration 48, loss = 4.00373322\n",
      "Iteration 49, loss = 3.99725885\n",
      "Iteration 50, loss = 3.96350680\n",
      "[CV] END ................................ score: (test=0.442) total time=   1.6s\n",
      "Iteration 1, loss = 18.46874678\n",
      "Iteration 2, loss = 5.63617617\n",
      "Iteration 3, loss = 6.25462541\n",
      "Iteration 4, loss = 6.25318943\n",
      "Iteration 5, loss = 6.14687896\n",
      "Iteration 6, loss = 6.01932770\n",
      "Iteration 7, loss = 5.90082684\n",
      "Iteration 8, loss = 5.79910506\n",
      "Iteration 9, loss = 5.69718564\n",
      "Iteration 10, loss = 5.60821683\n",
      "Iteration 11, loss = 5.52000612\n",
      "Iteration 12, loss = 5.44238287\n",
      "Iteration 13, loss = 5.36576592\n",
      "Iteration 14, loss = 5.29269648\n",
      "Iteration 15, loss = 5.22495486\n",
      "Iteration 16, loss = 5.16897452\n",
      "Iteration 17, loss = 5.09803324\n",
      "Iteration 18, loss = 5.03845784\n",
      "Iteration 19, loss = 4.98314367\n",
      "Iteration 20, loss = 4.92849536\n",
      "Iteration 21, loss = 4.88499041\n",
      "Iteration 22, loss = 4.82736105\n",
      "Iteration 23, loss = 4.77731833\n",
      "Iteration 24, loss = 4.72926440\n",
      "Iteration 25, loss = 4.68602086\n",
      "Iteration 26, loss = 4.64725962\n",
      "Iteration 27, loss = 4.60349002\n",
      "Iteration 28, loss = 4.56839929\n",
      "Iteration 29, loss = 4.52318368\n",
      "Iteration 30, loss = 4.48320015\n",
      "Iteration 31, loss = 4.44619330\n",
      "Iteration 32, loss = 4.41164027\n",
      "Iteration 33, loss = 4.37603659\n",
      "Iteration 34, loss = 4.34547391\n",
      "Iteration 35, loss = 4.31860283\n",
      "Iteration 36, loss = 4.28268716\n",
      "Iteration 37, loss = 4.25027771\n",
      "Iteration 38, loss = 4.22258240\n",
      "Iteration 39, loss = 4.19283797\n",
      "Iteration 40, loss = 4.16175511\n",
      "Iteration 41, loss = 4.13432364\n",
      "Iteration 42, loss = 4.10840684\n",
      "Iteration 43, loss = 4.08377782\n",
      "Iteration 44, loss = 4.06079040\n",
      "Iteration 45, loss = 4.03876477\n",
      "Iteration 46, loss = 4.01131151\n",
      "Iteration 47, loss = 4.00140850\n",
      "Iteration 48, loss = 3.97966337\n",
      "Iteration 49, loss = 3.94802745\n",
      "Iteration 50, loss = 3.92522152\n",
      "[CV] END ................................ score: (test=0.452) total time=   1.6s\n",
      "5-fold cross validation results: [0.29591837 0.41964286 0.48979592 0.44189017 0.45210728]\n",
      "\n",
      "algorithm MLP_3 accuracy results: mean = 0.419871 (std = 0.065997)\n",
      "\n",
      "all model trainings have been completed \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    7.9s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "k4folds = 5\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:   # Select each model in turn\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
    "        print(\" ++ NOW WORKING ON ALGORITHM %s ++\" % name)\n",
    "        print(\"Splitting data into %s folds\" % k4folds)\n",
    "        kfold = model_selection.KFold(n_splits=k4folds, random_state=42, shuffle=True)\n",
    "        print(\"Training model on each split ...\")\n",
    "        cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring, verbose=3)\n",
    "        print(\"5-fold cross validation results:\", cv_results)\n",
    "        print()\n",
    "        msg = \"algorithm %s %s results: mean = %f (std = %f)\" % (name, scoring, cv_results.mean(), cv_results.std())\n",
    "        print(msg)\n",
    "        print()\n",
    "print('all model trainings have been completed \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35491f7c-ecbf-444a-a087-2869eee78428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now tuning hyperparameters...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV 1/5] END alpha=0.01, max_iter=80, solver=sgd;, score=0.503 total time=   1.2s\n",
      "[CV 2/5] END alpha=0.01, max_iter=80, solver=sgd;, score=0.462 total time=   1.2s\n",
      "[CV 3/5] END alpha=0.01, max_iter=80, solver=sgd;, score=0.445 total time=   1.2s\n",
      "[CV 4/5] END alpha=0.01, max_iter=80, solver=sgd;, score=0.490 total time=   1.2s\n",
      "[CV 5/5] END alpha=0.01, max_iter=80, solver=sgd;, score=0.451 total time=   1.2s\n",
      "[CV 1/5] END alpha=0.01, max_iter=80, solver=adam;, score=0.485 total time=   1.3s\n",
      "[CV 2/5] END alpha=0.01, max_iter=80, solver=adam;, score=0.467 total time=   1.3s\n",
      "[CV 3/5] END alpha=0.01, max_iter=80, solver=adam;, score=0.510 total time=   1.1s\n",
      "[CV 4/5] END alpha=0.01, max_iter=80, solver=adam;, score=0.497 total time=   1.3s\n",
      "[CV 5/5] END alpha=0.01, max_iter=80, solver=adam;, score=0.496 total time=   1.3s\n",
      "[CV 1/5] END alpha=0.01, max_iter=100, solver=sgd;, score=0.481 total time=   1.5s\n",
      "[CV 2/5] END alpha=0.01, max_iter=100, solver=sgd;, score=0.462 total time=   1.5s\n",
      "[CV 3/5] END alpha=0.01, max_iter=100, solver=sgd;, score=0.480 total time=   1.1s\n",
      "[CV 4/5] END alpha=0.01, max_iter=100, solver=sgd;, score=0.497 total time=   1.5s\n",
      "[CV 5/5] END alpha=0.01, max_iter=100, solver=sgd;, score=0.455 total time=   1.5s\n",
      "[CV 1/5] END alpha=0.01, max_iter=100, solver=adam;, score=0.517 total time=   1.6s\n",
      "[CV 2/5] END alpha=0.01, max_iter=100, solver=adam;, score=0.486 total time=   1.7s\n",
      "[CV 3/5] END alpha=0.01, max_iter=100, solver=adam;, score=0.547 total time=   1.7s\n",
      "[CV 4/5] END alpha=0.01, max_iter=100, solver=adam;, score=0.496 total time=   1.1s\n",
      "[CV 5/5] END alpha=0.01, max_iter=100, solver=adam;, score=0.490 total time=   1.7s\n",
      "[CV 1/5] END alpha=0.05, max_iter=80, solver=sgd;, score=0.474 total time=   1.0s\n",
      "[CV 2/5] END alpha=0.05, max_iter=80, solver=sgd;, score=0.452 total time=   1.2s\n",
      "[CV 3/5] END alpha=0.05, max_iter=80, solver=sgd;, score=0.469 total time=   1.2s\n",
      "[CV 4/5] END alpha=0.05, max_iter=80, solver=sgd;, score=0.478 total time=   1.2s\n",
      "[CV 5/5] END alpha=0.05, max_iter=80, solver=sgd;, score=0.446 total time=   1.2s\n",
      "[CV 1/5] END alpha=0.05, max_iter=80, solver=adam;, score=0.463 total time=   1.3s\n",
      "[CV 2/5] END alpha=0.05, max_iter=80, solver=adam;, score=0.476 total time=   1.3s\n",
      "[CV 3/5] END alpha=0.05, max_iter=80, solver=adam;, score=0.533 total time=   1.3s\n",
      "[CV 4/5] END alpha=0.05, max_iter=80, solver=adam;, score=0.503 total time=   1.3s\n",
      "[CV 5/5] END alpha=0.05, max_iter=80, solver=adam;, score=0.469 total time=   1.3s\n",
      "[CV 1/5] END alpha=0.05, max_iter=100, solver=sgd;, score=0.460 total time=   1.5s\n",
      "[CV 2/5] END alpha=0.05, max_iter=100, solver=sgd;, score=0.462 total time=   1.5s\n",
      "[CV 3/5] END alpha=0.05, max_iter=100, solver=sgd;, score=0.485 total time=   1.5s\n",
      "[CV 4/5] END alpha=0.05, max_iter=100, solver=sgd;, score=0.483 total time=   1.5s\n",
      "[CV 5/5] END alpha=0.05, max_iter=100, solver=sgd;, score=0.462 total time=   1.5s\n",
      "[CV 1/5] END alpha=0.05, max_iter=100, solver=adam;, score=0.515 total time=   1.7s\n",
      "[CV 2/5] END alpha=0.05, max_iter=100, solver=adam;, score=0.505 total time=   1.6s\n",
      "[CV 3/5] END alpha=0.05, max_iter=100, solver=adam;, score=0.537 total time=   1.5s\n",
      "[CV 4/5] END alpha=0.05, max_iter=100, solver=adam;, score=0.475 total time=   1.3s\n",
      "[CV 5/5] END alpha=0.05, max_iter=100, solver=adam;, score=0.473 total time=   1.7s\n",
      "Best hyperparameters found on development set:\n",
      "{'alpha': 0.01, 'max_iter': 100, 'solver': 'adam'}\n",
      "Grid scores on development set:\n",
      "0.470 (+/-0.045) for {'alpha': 0.01, 'max_iter': 80, 'solver': 'sgd'}\n",
      "0.491 (+/-0.029) for {'alpha': 0.01, 'max_iter': 80, 'solver': 'adam'}\n",
      "0.475 (+/-0.030) for {'alpha': 0.01, 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.507 (+/-0.045) for {'alpha': 0.01, 'max_iter': 100, 'solver': 'adam'}\n",
      "0.464 (+/-0.026) for {'alpha': 0.05, 'max_iter': 80, 'solver': 'sgd'}\n",
      "0.489 (+/-0.052) for {'alpha': 0.05, 'max_iter': 80, 'solver': 'adam'}\n",
      "0.470 (+/-0.022) for {'alpha': 0.05, 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.501 (+/-0.049) for {'alpha': 0.05, 'max_iter': 100, 'solver': 'adam'}\n",
      "done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### QUESTION 2 - Modifying MLP Optimizers using GridSearch\n",
    "\n",
    "# Tuning Decision Tree Classifier hyperparameters using GridSearch\n",
    "\n",
    "selected_model = MLPClassifier()\n",
    "hyperparameters = {'solver': ['sgd', 'adam'], 'alpha':[0.01, 0.05], 'max_iter': [80, 100]}\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
    "    print(\"Now tuning hyperparameters...\")\n",
    "    clf = GridSearchCV(selected_model, hyperparameters, cv=5, scoring=scoring, verbose=4)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best hyperparameters found on development set:\")\n",
    "    print(clf.best_params_)\n",
    "    print(\"Grid scores on development set:\")\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "            % (mean, std * 2, params))\n",
    "    print('done \\n')\n",
    "\n",
    "    MLP_tuned_model = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df295fb7-f710-4602-8c91-3ad0f61a7f56",
   "metadata": {},
   "source": [
    "#### Through the use of Grid search, the variation of the optimizer parameters influenced the performance as shown above. The best optimizer parameters found on the development set include an MLP(alpha = 0.01, learning_rate_init = 0.02, max_iter (epoch) = 100, and solver = adam). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2aee5b9-2b7a-43a8-95e8-fe654e872b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now testing the best tuned model on the separate test set...\n",
      "Detailed classification report:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         5\n",
      "           4       0.50      0.08      0.14        25\n",
      "           5       0.49      0.70      0.58       291\n",
      "           6       0.50      0.59      0.54       432\n",
      "           7       0.46      0.11      0.18       192\n",
      "           8       0.00      0.00      0.00        35\n",
      "\n",
      "    accuracy                           0.49       980\n",
      "   macro avg       0.32      0.25      0.24       980\n",
      "weighted avg       0.47      0.49      0.45       980\n",
      "\n",
      "Cohen Kappa Score: 0.19728829139377224\n",
      "done \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/dsi/apps/anaconda3/python-3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/project/dsi/apps/anaconda3/python-3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/project/dsi/apps/anaconda3/python-3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 3 - TESTING THE PERFORMANCE OF THE BEST MODEL\n",
    "\n",
    "print(\"Now testing the best tuned model on the separate test set...\")\n",
    "print(\"Detailed classification report:\")\n",
    "print('\\n')\n",
    "y_true, y_pred = y_test, MLP_tuned_model.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print('Cohen Kappa Score:', cohen_kappa_score(y_true, y_pred))\n",
    "print('done \\n')\n",
    "\n",
    "#print(f'Tuned decision tree has {tuned_model.tree_.node_count} nodes with maximum depth {tuned_model.tree_.max_depth}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3286a322-500a-4d10-81fd-a7092c070787",
   "metadata": {},
   "source": [
    "##### As expected, the tuned model accuracy on the test data closely match that obtained from the trained model which shows that overfitting is very limited. \n",
    "##### Also, from inspection, the 51% accuracy can be termed fair. \n",
    "\n",
    "##### PS: the accuracy is slightly unstable as it varies slightly whenever it is re-run (so restarting the kernel might chane the answer between + or - 1 to 2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7996c597-b2f1-43a2-887a-e31376b0c6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now tuning hyperparameters...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV 1/5] END .............C=0.01, kernel=linear;, score=0.511 total time=   4.9s\n",
      "[CV 2/5] END .............C=0.01, kernel=linear;, score=0.497 total time=   5.5s\n",
      "[CV 3/5] END .............C=0.01, kernel=linear;, score=0.515 total time=   6.0s\n",
      "[CV 4/5] END .............C=0.01, kernel=linear;, score=0.494 total time=   7.2s\n",
      "[CV 5/5] END .............C=0.01, kernel=linear;, score=0.484 total time=   7.1s\n",
      "[CV 1/5] END ................C=0.01, kernel=rbf;, score=0.452 total time=   1.0s\n",
      "[CV 2/5] END ................C=0.01, kernel=rbf;, score=0.450 total time=   1.0s\n",
      "[CV 3/5] END ................C=0.01, kernel=rbf;, score=0.450 total time=   1.0s\n",
      "[CV 4/5] END ................C=0.01, kernel=rbf;, score=0.451 total time=   1.0s\n",
      "[CV 5/5] END ................C=0.01, kernel=rbf;, score=0.451 total time=   1.0s\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 4 - training a different classifier \n",
    "# Tuning Support Vector Machine Classifier hyperparameters using GridSearch\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "selected_model = SVC()\n",
    "hyperparameters = {'kernel':['linear', 'rbf'], 'C':[0.01, 1] }\n",
    "\n",
    "print(\"Now tuning hyperparameters...\")\n",
    "clf = GridSearchCV(selected_model, hyperparameters, cv=5, scoring=scoring, verbose=4)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Best hyperparameters found on development set:\")\n",
    "print(clf.best_params_)\n",
    "print(\"Grid scores on development set:\")\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "        % (mean, std * 2, params))\n",
    "print('done \\n')\n",
    "SVC_tuned_model = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa53fdf-4c67-4858-aa42-bffa0a928d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 4 - TESTING THE PERFORMANCE OF THE SVC BEST MODEL\n",
    "\n",
    "print(\"Now testing the best tuned model on the separate test set...\")\n",
    "print(\"Detailed classification report:\")\n",
    "print('\\n')\n",
    "y_true, y_pred = y_test, SVC_tuned_model.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print('Cohen Kappa Score:', cohen_kappa_score(y_true, y_pred))\n",
    "print('done \\n')\n",
    "\n",
    "#print(f'Tuned decision tree has {tuned_model.tree_.node_count} nodes with maximum depth {tuned_model.tree_.max_depth}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a96a1c-aa5d-410f-afbd-ebf12fd927e5",
   "metadata": {},
   "source": [
    "### From inspection, the SVC model with 51% which turns out to be the same as the MLP model with 50% accuracy. \n",
    "### It is worth to note that the SVC model training were less interrupted when compared to the MLP model which had covergence warnings due to the limited number of iterations. With further training, I believe the MLP model could beat the performance of the SVC model.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
